p8105_hw5_sy3352
================
Su Yan
2025-11-05

## Problem 1

``` r
same_bday = function(n) {
  generated_bday = sample(1:365, n, replace = TRUE)
  tibble(duplicate_bday = any(duplicated(generated_bday)))
}

sim_results_df = 
  expand_grid(
    sample_size = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    estimate_df = map(sample_size, same_bday)
  ) |> 
  unnest(estimate_df)

same_bday_prob = sim_results_df |> 
  group_by(sample_size) |> 
  summarize(same_bday_probability = mean(duplicate_bday))

same_bday_prob_pt = same_bday_prob |> 
  ggplot(aes(x = sample_size,
             y = same_bday_probability)) + 
  geom_line()
same_bday_prob_pt
```

![](p8105_hw5_sy3352_files/figure-gfm/unnamed-chunk-1-1.png)<!-- -->
From the plot and the simulated birthday data, we can see that the as
sample size get bigger, the probability to have same birthday in the
group also get bigger at the same time. As sample size gets close to 50,
the probability of having same birthday in the group gets very close to
1, which means we can almost be certain that there will be two person
having the same birthday once the group is bigger than 50.

## Problem 2

``` r
each_sample_test = function(mean_repeat) {
  x = rnorm(30, mean = mean_repeat, sd = 5)
  t.test(x, mu = 0, alternative = c("two.sided"), conf.level = 0.95) |> 
    broom::tidy()
}

sim_normal_datasets = 
  expand_grid(
    mean_repeat = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    sim_normal_df = map(mean_repeat, each_sample_test)) |> 
  unnest(sim_normal_df)

normal_results = sim_normal_datasets |> 
  select(mean_repeat, estimate, p.value) |> 
  mutate(power = if_else(p.value < 0.05, "Reject", "Fail")) 

power_prob = normal_results |> 
  select(-p.value) |> 
  group_by(mean_repeat) |> 
  summarize(estimate_mean = mean(estimate),
            power_proportion = mean(power == "Reject"))

effect_size_power_pt = power_prob |> 
  ggplot(aes(x = mean_repeat,
             y = power_proportion)) + 
  geom_point() + 
  geom_line() +
  scale_x_continuous(breaks = 0:6) +
  labs(x = "True value of μ", y = "Power of the test")
effect_size_power_pt
```

![](p8105_hw5_sy3352_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->
From the plot above, we can see that as the true value of μ gets bigger,
the power (proportion of times the null was rejected) drastically
increased and slowed down when μ is bigger than 3. When μ reaches 4, the
power get almost 1 and stays. Such observation implies that at
significance level 0.05, we have higher probability to reject H0 with
bigger true μ.

``` r
estimate_true_mean_pt = power_prob |> 
  ggplot(aes(x = mean_repeat,
             y = estimate_mean)) + 
  geom_point() + 
  geom_line() +
  scale_x_continuous(breaks = 0:6) +
  scale_y_continuous(breaks = 0:6) +
  labs(x = "True value of μ", y = "Estimated value of μ")
estimate_true_mean_pt
```

![](p8105_hw5_sy3352_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

``` r
estimate_mean_reject_pt = normal_results |> 
  select(-p.value) |> 
  filter(power == "Reject") |> 
  group_by(mean_repeat) |> 
  summarize(estimate_mean = mean(estimate)) |> 
  ggplot(aes(x = mean_repeat,
             y = estimate_mean)) + 
  geom_point() + 
  geom_line() +
  scale_x_continuous(breaks = 0:6) +
  scale_y_continuous(breaks = 0:6) +
  labs(x = "True value of μ", y = "Estimated value of μ (when null is rejected)")
estimate_mean_reject_pt
```

![](p8105_hw5_sy3352_files/figure-gfm/unnamed-chunk-3-2.png)<!-- -->
From the plots, we can see that the estimated mean is almost the same as
the true value of mean for all values in our simulation. When looking at
the estimated value of mean only in the samples for which the null was
rejected, the estimated mean is higher than the true value of mean from
0 to 3, and then get almost the same starting from 4. This is because
when power is low, only the extreme cases are selected for our plot,
thus over-estimating the mean.

## Problem 3

``` r
homicides_df = read_csv("data/homicide-data.csv") |> 
  mutate(location = sprintf("%s, %s", city, state),
         unsolved = if_else(disposition == "Closed by arrest", 0, 1)) |> 
  select(location, unsolved) 
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
homicides_summary = homicides_df |> 
  group_by(location) |> 
  summarize(total_homicides = n(),
            unsolved_homicides = sum(unsolved))
homicides_summary
```

    ## # A tibble: 51 × 3
    ##    location        total_homicides unsolved_homicides
    ##    <chr>                     <int>              <dbl>
    ##  1 Albuquerque, NM             378                146
    ##  2 Atlanta, GA                 973                373
    ##  3 Baltimore, MD              2827               1825
    ##  4 Baton Rouge, LA             424                196
    ##  5 Birmingham, AL              800                347
    ##  6 Boston, MA                  614                310
    ##  7 Buffalo, NY                 521                319
    ##  8 Charlotte, NC               687                206
    ##  9 Chicago, IL                5535               4073
    ## 10 Cincinnati, OH              694                309
    ## # ℹ 41 more rows

The homicide raw data contains 52179 rows of data, which covered the
homicide cases from 2007 to 2017 in 50 US cities. Each row represents a
homicide case. Some of the key variables are `reported_date`,
`victim_race`, `victim_age`, `victim_sex`, `city`, `state`,
`disposition`. There are 3 different dispositions: `Open/No arrest`,
`Closed by arrest`, `Closed without arrest`.

``` r
baltimore_test = homicides_df |> 
  filter(location == "Baltimore, MD") 
baltimore_result = broom::tidy(
    prop.test(x = sum(baltimore_test$unsolved), n = nrow(baltimore_test), 
              conf.level = 0.95, correct = TRUE))
baltimore_result |> pull(estimate) 
```

    ##         p 
    ## 0.6455607

``` r
baltimore_result |> pull(conf.low) 
```

    ## [1] 0.6275625

``` r
baltimore_result |> pull(conf.high) 
```

    ## [1] 0.6631599

``` r
all_city_test = homicides_summary |> 
  mutate(unsolved_proportion = unsolved_homicides/total_homicides) |> 
  mutate(test_summary = purrr::map2(unsolved_homicides, total_homicides, ~prop.test(x = .x, n = .y, conf.level = 0.95, correct = TRUE)),
         test_tidy = map(test_summary, broom::tidy)
         ) |> 
  unnest(test_tidy)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `test_summary = purrr::map2(...)`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
all_city_test_tidy = all_city_test |> 
  select(location, unsolved_proportion, estimate, conf.low, conf.high)

all_city_test_pt = all_city_test_tidy |> 
  ggplot(aes(x = reorder(location, unsolved_proportion), y = estimate)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(x = "City", y = "Estimated proportion of unsolved homicides (95% CI)")
all_city_test_pt
```

![](p8105_hw5_sy3352_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->
